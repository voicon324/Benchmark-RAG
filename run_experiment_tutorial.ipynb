{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f770214",
   "metadata": {},
   "source": [
    "# NewAIBench Experiment Runner Tutorial\n",
    "\n",
    "Notebook n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc ƒë·ªÉ ch·∫°y experiment v·ªõi NewAIBench framework, t·ª´ vi·ªác pull git repository ƒë·∫øn khi ho√†n th√†nh experiment.\n",
    "\n",
    "## T·ªïng quan\n",
    "- Clone repository v√† c·∫≠p nh·∫≠t code m·ªõi nh·∫•t\n",
    "- C√†i ƒë·∫∑t dependencies\n",
    "- T·∫°o custom YAML configuration\n",
    "- Ch·∫°y experiment v√† ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "‚ö†Ô∏è **L∆∞u √Ω**: Notebook n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y trong environment ƒë√£ c√≥ Python v√† Git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Thi·∫øt l·∫≠p working directory\n",
    "WORK_DIR = \"/home/hkduy/NewAI/new_bench\"\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e056da",
   "metadata": {},
   "source": [
    "## 1. Clone Repository v√† C·∫≠p nh·∫≠t Code\n",
    "\n",
    "ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω clone repository (n·∫øu ch∆∞a c√≥) ho·∫∑c pull code m·ªõi nh·∫•t t·ª´ remote repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e89531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra xem ƒë√£ c√≥ git repository ch∆∞a\n",
    "if os.path.exists('.git'):\n",
    "    print(\"‚úÖ Git repository ƒë√£ t·ªìn t·∫°i\")\n",
    "    # Pull latest changes\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'status', '--porcelain'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        if result.stdout.strip():\n",
    "            print(\"‚ö†Ô∏è C√≥ changes ch∆∞a commit:\")\n",
    "            print(result.stdout)\n",
    "            print(\"\\nB·∫°n c√≥ th·ªÉ stash changes tr∆∞·ªõc khi pull:\")\n",
    "            print(\"git stash\")\n",
    "        else:\n",
    "            print(\"Working directory clean, pulling latest changes...\")\n",
    "            pull_result = subprocess.run(['git', 'pull'], \n",
    "                                       capture_output=True, text=True, check=True)\n",
    "            print(f\"‚úÖ Git pull completed: {pull_result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Git operation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y git repository\")\n",
    "    print(\"N·∫øu b·∫°n c·∫ßn clone repository, uncomment v√† ch·∫°y l·ªánh d∆∞·ªõi ƒë√¢y:\")\n",
    "    print(\"# git clone <repository_url> .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã th√¥ng tin branch hi·ªán t·∫°i\n",
    "try:\n",
    "    branch_result = subprocess.run(['git', 'branch', '--show-current'], \n",
    "                                 capture_output=True, text=True, check=True)\n",
    "    current_branch = branch_result.stdout.strip()\n",
    "    print(f\"üîÑ Current branch: {current_branch}\")\n",
    "    \n",
    "    # Hi·ªÉn th·ªã commit g·∫ßn nh·∫•t\n",
    "    commit_result = subprocess.run(['git', 'log', '-1', '--oneline'], \n",
    "                                 capture_output=True, text=True, check=True)\n",
    "    latest_commit = commit_result.stdout.strip()\n",
    "    print(f\"üìù Latest commit: {latest_commit}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Cannot get git info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b686536",
   "metadata": {},
   "source": [
    "## 2. C√†i ƒë·∫∑t Dependencies\n",
    "\n",
    "C√†i ƒë·∫∑t t·∫•t c·∫£ c√°c dependencies c·∫ßn thi·∫øt cho NewAIBench framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3949138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra Python version\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìÇ Python executable: {sys.executable}\")\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ requirements.txt kh√¥ng\n",
    "if os.path.exists('requirements.txt'):\n",
    "    print(\"‚úÖ Found requirements.txt\")\n",
    "    with open('requirements.txt', 'r') as f:\n",
    "        requirements = f.read()\n",
    "    print(\"\\nüìã Requirements:\")\n",
    "    print(requirements[:500] + \"...\" if len(requirements) > 500 else requirements)\n",
    "else:\n",
    "    print(\"‚ùå requirements.txt not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t dependencies\n",
    "print(\"üîß Installing dependencies...\")\n",
    "try:\n",
    "    # Upgrade pip first\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'], \n",
    "                  check=True)\n",
    "    print(\"‚úÖ Pip upgraded\")\n",
    "    \n",
    "    # Install requirements\n",
    "    if os.path.exists('requirements.txt'):\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"‚úÖ Requirements installed successfully\")\n",
    "        if result.stderr:\n",
    "            print(f\"‚ö†Ô∏è Warnings: {result.stderr[:200]}...\")\n",
    "    \n",
    "    # Install package in development mode\n",
    "    if os.path.exists('setup.py'):\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], \n",
    "                      check=True)\n",
    "        print(\"‚úÖ Package installed in development mode\")\n",
    "        \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(f\"Error output: {e.stderr if hasattr(e, 'stderr') else 'No error details'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra xem NewAIBench ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ch∆∞a\n",
    "try:\n",
    "    # Add src to path if needed\n",
    "    src_path = os.path.join(os.getcwd(), \"src\")\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    \n",
    "    # Test import\n",
    "    from newaibench.experiment import ExperimentRunner, ExperimentConfig\n",
    "    print(\"‚úÖ NewAIBench import successful\")\n",
    "    \n",
    "    # Check if run_experiment.py exists\n",
    "    if os.path.exists('run_experiment.py'):\n",
    "        print(\"‚úÖ run_experiment.py found\")\n",
    "    else:\n",
    "        print(\"‚ùå run_experiment.py not found\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"C√≥ th·ªÉ c·∫ßn c√†i ƒë·∫∑t th√™m dependencies ho·∫∑c ki·ªÉm tra PYTHONPATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65128d",
   "metadata": {},
   "source": [
    "## 3. T·∫°o Custom YAML Configuration\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω t·∫°o m·ªôt file c·∫•u h√¨nh YAML t√πy ch·ªânh cho experiment c·ªßa b·∫°n.\n",
    "\n",
    "### 3.1 Xem c√°c template c√≥ s·∫µn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Li·ªát k√™ c√°c experiment templates c√≥ s·∫µn\n",
    "print(\"üìÇ Existing experiment configurations:\")\n",
    "print(\"\\n1. In examples/experiments/:\")\n",
    "examples_exp_dir = Path(\"examples/experiments\")\n",
    "if examples_exp_dir.exists():\n",
    "    for yaml_file in examples_exp_dir.glob(\"*.yaml\"):\n",
    "        print(f\"   - {yaml_file.name}\")\n",
    "    for json_file in examples_exp_dir.glob(\"*.json\"):\n",
    "        print(f\"   - {json_file.name}\")\n",
    "else:\n",
    "    print(\"   (directory not found)\")\n",
    "\n",
    "print(\"\\n2. In experiments/:\")\n",
    "exp_dir = Path(\"experiments\")\n",
    "if exp_dir.exists():\n",
    "    for yaml_file in exp_dir.glob(\"*.yaml\"):\n",
    "        print(f\"   - {yaml_file.name}\")\n",
    "    for json_file in exp_dir.glob(\"*.json\"):\n",
    "        print(f\"   - {json_file.name}\")\n",
    "else:\n",
    "    print(\"   (directory not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de22add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o template m·∫´u b·∫±ng CLI tool\n",
    "print(\"üîß Creating a basic template...\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, 'run_experiment.py', \n",
    "                           '--create-template', 'basic', \n",
    "                           '--output-config', 'my_custom_experiment.yaml'], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ Template created successfully\")\n",
    "    print(f\"Output: {result.stdout}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Template creation failed: {e}\")\n",
    "    print(f\"Error: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã n·ªôi dung template v·ª´a t·∫°o\n",
    "if os.path.exists('my_custom_experiment.yaml'):\n",
    "    print(\"üìù Generated template content:\")\n",
    "    with open('my_custom_experiment.yaml', 'r') as f:\n",
    "        template_content = f.read()\n",
    "    print(template_content)\n",
    "else:\n",
    "    print(\"‚ùå Template file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea28d53",
   "metadata": {},
   "source": [
    "### 3.2 T√πy ch·ªânh Configuration\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω t√πy ch·ªânh file YAML theo nhu c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a tr·ª±c ti·∫øp file ho·∫∑c t·∫°o m·ªôt file m·ªõi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o custom YAML configuration\n",
    "custom_config = {\n",
    "    'description': 'Custom experiment for testing NewAIBench',\n",
    "    'models': [\n",
    "        {\n",
    "            'name': 'bm25_model',\n",
    "            'type': 'sparse',\n",
    "            'model_name_or_path': '',  # BM25 kh√¥ng c·∫ßn model path\n",
    "            'parameters': {\n",
    "                'k1': 1.2,\n",
    "                'b': 0.75\n",
    "            },\n",
    "            'device': 'cpu',\n",
    "            'batch_size': 32\n",
    "        }\n",
    "    ],\n",
    "    'datasets': [\n",
    "        {\n",
    "            'name': 'test_dataset',\n",
    "            'type': 'text',\n",
    "            'data_dir': './BKAI_law_data/newaibench_formatted_data/legal_data',  # S·ª≠ d·ª•ng dataset c√≥ s·∫µn\n",
    "            'config_overrides': {\n",
    "                'cache_enabled': True\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    'evaluation': {\n",
    "        'metrics': ['ndcg', 'map', 'recall'],\n",
    "        'k_values': [1, 5, 10],\n",
    "        'top_k': 100,\n",
    "        'save_run_file': True\n",
    "    },\n",
    "    'output': {\n",
    "        'output_dir': './results',\n",
    "        'experiment_name': f'custom_experiment_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "        'log_level': 'INFO',\n",
    "        'overwrite': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# L∆∞u v√†o file YAML\n",
    "with open('my_custom_experiment.yaml', 'w') as f:\n",
    "    yaml.dump(custom_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Custom YAML configuration created:\")\n",
    "with open('my_custom_experiment.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration file\n",
    "print(\"üîç Validating configuration...\")\n",
    "try:\n",
    "    # Test loading configuration\n",
    "    from newaibench.experiment import load_experiment_config\n",
    "    config = load_experiment_config('my_custom_experiment.yaml')\n",
    "    print(\"‚úÖ Configuration file is valid\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nüìä Configuration Summary:\")\n",
    "    print(f\"   - Models: {len(config.models)}\")\n",
    "    print(f\"   - Datasets: {len(config.datasets)}\")\n",
    "    print(f\"   - Metrics: {', '.join(config.evaluation.metrics)}\")\n",
    "    print(f\"   - Output: {config.output.output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration validation failed: {e}\")\n",
    "    print(\"Vui l√≤ng ki·ªÉm tra l·∫°i file YAML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9344c",
   "metadata": {},
   "source": [
    "### 3.3 Ki·ªÉm tra Dataset\n",
    "\n",
    "Tr∆∞·ªõc khi ch·∫°y experiment, h√£y ki·ªÉm tra xem dataset c√≥ t·ªìn t·∫°i v√† ƒë√∫ng format kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra dataset\n",
    "dataset_path = './BKAI_law_data/newaibench_formatted_data/legal_data'\n",
    "print(f\"üîç Checking dataset at: {dataset_path}\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"‚úÖ Dataset directory exists\")\n",
    "    \n",
    "    # List contents\n",
    "    contents = list(Path(dataset_path).iterdir())\n",
    "    print(f\"üìÅ Dataset contains {len(contents)} items:\")\n",
    "    for item in contents[:10]:  # Show first 10 items\n",
    "        print(f\"   - {item.name}\")\n",
    "    if len(contents) > 10:\n",
    "        print(f\"   ... and {len(contents) - 10} more items\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['corpus.jsonl', 'queries.jsonl', 'qrels.jsonl']\n",
    "    for req_file in required_files:\n",
    "        file_path = Path(dataset_path) / req_file\n",
    "        if file_path.exists():\n",
    "            print(f\"‚úÖ Found {req_file} ({file_path.stat().st_size} bytes)\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing {req_file}\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset directory not found\")\n",
    "    print(\"Available datasets in BKAI_law_data/:\")\n",
    "    if os.path.exists('./BKAI_law_data'):\n",
    "        for item in Path('./BKAI_law_data').iterdir():\n",
    "            print(f\"   - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0fbeb",
   "metadata": {},
   "source": [
    "## 4. Ch·∫°y Experiment\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω ch·∫°y experiment v·ªõi c·∫•u h√¨nh ƒë√£ t·∫°o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y dry-run tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra\n",
    "print(\"üß™ Running dry-run to validate setup...\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, 'run_experiment.py', \n",
    "                           '--config', 'my_custom_experiment.yaml',\n",
    "                           '--dry-run'], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ Dry-run completed successfully\")\n",
    "    print(\"üìã Execution plan:\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Dry-run failed: {e}\")\n",
    "    print(f\"Error output: {e.stderr}\")\n",
    "    print(\"\\nVui l√≤ng ki·ªÉm tra l·∫°i configuration v√† dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2021017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y experiment th·ª±c t·∫ø\n",
    "print(\"üöÄ Starting actual experiment...\")\n",
    "print(\"‚ö†Ô∏è Experiment n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ ho√†n th√†nh\")\n",
    "print(\"\\nN·∫øu b·∫°n mu·ªën ch·∫°y experiment, uncomment d√≤ng code d∆∞·ªõi ƒë√¢y:\")\n",
    "print(\"\\n# Ch·∫°y experiment (uncomment ƒë·ªÉ th·ª±c thi)\")\n",
    "run_experiment = False  # ƒê·∫∑t th√†nh True n·∫øu mu·ªën ch·∫°y ngay\n",
    "\n",
    "if run_experiment:\n",
    "    try:\n",
    "        # T·∫°o process ƒë·ªÉ ch·∫°y experiment\n",
    "        process = subprocess.Popen([sys.executable, 'run_experiment.py', \n",
    "                                  '--config', 'my_custom_experiment.yaml'],\n",
    "                                 stdout=subprocess.PIPE,\n",
    "                                 stderr=subprocess.PIPE,\n",
    "                                 text=True,\n",
    "                                 bufsize=1,\n",
    "                                 universal_newlines=True)\n",
    "        \n",
    "        print(\"Experiment ƒëang ch·∫°y...\")\n",
    "        \n",
    "        # Real-time output\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "        \n",
    "        # Get final result\n",
    "        return_code = process.poll()\n",
    "        stderr_output = process.stderr.read()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"\\n‚úÖ Experiment completed successfully!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Experiment failed with return code {return_code}\")\n",
    "            if stderr_output:\n",
    "                print(f\"Error: {stderr_output}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to run experiment: {e}\")\n",
    "else:\n",
    "    print(\"\\nƒê·ªÉ ch·∫°y experiment, b·∫°n c√≥ th·ªÉ:\")\n",
    "    print(\"1. ƒê·∫∑t run_experiment = True ·ªü cell tr√™n\")\n",
    "    print(\"2. Ho·∫∑c ch·∫°y command line: python run_experiment.py --config my_custom_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7bd81",
   "metadata": {},
   "source": [
    "### 4.1 Ch·∫°y Experiment Manually (T√πy ch·ªçn)\n",
    "\n",
    "N·∫øu b·∫°n mu·ªën ch·∫°y experiment t·ª´ command line ho·∫∑c c√≥ ki·ªÉm so√°t nhi·ªÅu h∆°n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã command ƒë·ªÉ ch·∫°y experiment\n",
    "config_file = 'my_custom_experiment.yaml'\n",
    "command = f\"python run_experiment.py --config {config_file}\"\n",
    "\n",
    "print(\"üîß Command ƒë·ªÉ ch·∫°y experiment:\")\n",
    "print(f\"cd {os.getcwd()}\")\n",
    "print(command)\n",
    "print()\n",
    "print(\"üìã C√°c options kh√°c:\")\n",
    "print(f\"# Ch·∫°y dry-run: python run_experiment.py --config {config_file} --dry-run\")\n",
    "print(f\"# V·ªõi log debug: python run_experiment.py --config {config_file} --log-level DEBUG\")\n",
    "print(f\"# Overwrite results: python run_experiment.py --config {config_file} --overwrite\")\n",
    "\n",
    "# Copy command to clipboard (if possible)\n",
    "try:\n",
    "    import pyperclip\n",
    "    pyperclip.copy(command)\n",
    "    print(\"\\n‚úÖ Command copied to clipboard!\")\n",
    "except ImportError:\n",
    "    print(\"\\nüí° Tip: Install pyperclip ƒë·ªÉ auto-copy command: pip install pyperclip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951ac53",
   "metadata": {},
   "source": [
    "## 5. Ki·ªÉm tra K·∫øt qu·∫£\n",
    "\n",
    "Sau khi experiment ho√†n th√†nh, ch√∫ng ta s·∫Ω ki·ªÉm tra k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8392e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra k·∫øt qu·∫£ experiment\n",
    "results_dir = './results'\n",
    "print(f\"üîç Checking results in: {results_dir}\")\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"‚úÖ Results directory exists\")\n",
    "    \n",
    "    # List experiment directories\n",
    "    experiments = [d for d in Path(results_dir).iterdir() if d.is_dir()]\n",
    "    print(f\"\\nüìä Found {len(experiments)} experiment(s):\")\n",
    "    \n",
    "    for exp_dir in sorted(experiments, key=lambda x: x.stat().st_mtime, reverse=True):\n",
    "        print(f\"\\nüìÅ {exp_dir.name}:\")\n",
    "        contents = list(exp_dir.iterdir())\n",
    "        for item in contents:\n",
    "            if item.is_file():\n",
    "                size = item.stat().st_size\n",
    "                print(f\"   üìÑ {item.name} ({size} bytes)\")\n",
    "            else:\n",
    "                print(f\"   üìÅ {item.name}/\")\n",
    "        \n",
    "        # Check for evaluation results\n",
    "        eval_file = exp_dir / 'evaluation_results.json'\n",
    "        if eval_file.exists():\n",
    "            print(f\"   ‚úÖ Found evaluation results\")\n",
    "            try:\n",
    "                with open(eval_file, 'r') as f:\n",
    "                    eval_data = json.load(f)\n",
    "                print(f\"   üìà {len(eval_data)} result(s) available\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Could not read evaluation results: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Results directory not found\")\n",
    "    print(\"Experiment ch∆∞a ƒë∆∞·ª£c ch·∫°y ho·∫∑c ch∆∞a ho√†n th√†nh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5710cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v√† hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt (n·∫øu c√≥)\n",
    "def display_evaluation_results(results_dir):\n",
    "    experiments = [d for d in Path(results_dir).iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not experiments:\n",
    "        print(\"‚ùå No experiment results found\")\n",
    "        return\n",
    "    \n",
    "    # Get latest experiment\n",
    "    latest_exp = max(experiments, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"üìä Latest experiment: {latest_exp.name}\")\n",
    "    \n",
    "    eval_file = latest_exp / 'evaluation_results.json'\n",
    "    if eval_file.exists():\n",
    "        try:\n",
    "            with open(eval_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            print(\"\\nüìà Evaluation Results:\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for result in results:\n",
    "                model_name = result.get('model', 'Unknown')\n",
    "                dataset_name = result.get('dataset', 'Unknown')\n",
    "                metrics = result.get('metrics', {})\n",
    "                \n",
    "                print(f\"\\nü§ñ Model: {model_name}\")\n",
    "                print(f\"üìö Dataset: {dataset_name}\")\n",
    "                print(f\"üìä Metrics:\")\n",
    "                \n",
    "                for metric_name, values in metrics.items():\n",
    "                    print(f\"   {metric_name}:\")\n",
    "                    if isinstance(values, dict):\n",
    "                        for k, v in values.items():\n",
    "                            print(f\"     @{k}: {v:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"     {values:.4f}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading results: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No evaluation_results.json found\")\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    display_evaluation_results(results_dir)\n",
    "else:\n",
    "    print(\"Ch∆∞a c√≥ k·∫øt qu·∫£ ƒë·ªÉ hi·ªÉn th·ªã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d97385",
   "metadata": {},
   "source": [
    "## 6. T√≥m t·∫Øt v√† B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "üéâ **Ch√∫c m·ª´ng!** B·∫°n ƒë√£ ho√†n th√†nh tutorial s·ª≠ d·ª•ng NewAIBench.\n",
    "\n",
    "### Nh·ªØng g√¨ ƒë√£ l√†m:\n",
    "1. ‚úÖ Clone/update repository\n",
    "2. ‚úÖ C√†i ƒë·∫∑t dependencies\n",
    "3. ‚úÖ T·∫°o custom YAML configuration\n",
    "4. ‚úÖ Ch·∫°y experiment (ho·∫∑c chu·∫©n b·ªã ƒë·ªÉ ch·∫°y)\n",
    "5. ‚úÖ Ki·ªÉm tra k·∫øt qu·∫£\n",
    "\n",
    "### B∆∞·ªõc ti·∫øp theo:\n",
    "1. **T√πy ch·ªânh th√™m**: Ch·ªânh s·ª≠a file YAML ƒë·ªÉ th·ª≠ nghi·ªám v·ªõi c√°c models v√† datasets kh√°c\n",
    "2. **Ph√¢n t√≠ch k·∫øt qu·∫£**: S·ª≠ d·ª•ng c√°c tools c√≥ s·∫µn ƒë·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ chi ti·∫øt\n",
    "3. **Ch·∫°y nhi·ªÅu experiments**: So s√°nh hi·ªáu su·∫•t c·ªßa c√°c models kh√°c nhau\n",
    "4. **T·ªëi ∆∞u h√≥a**: ƒêi·ªÅu ch·ªânh parameters ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£\n",
    "\n",
    "### T√†i li·ªáu tham kh·∫£o:\n",
    "- üìö **Documentation**: `docs/` directory\n",
    "- üß™ **Examples**: `examples/` directory  \n",
    "- ‚öôÔ∏è **Configuration**: `examples/experiments/` directory\n",
    "\n",
    "### L∆∞u √Ω quan tr·ªçng:\n",
    "- ‚ö†Ô∏è Lu√¥n ch·∫°y `--dry-run` tr∆∞·ªõc khi ch·∫°y experiment th·ª±c t·∫ø\n",
    "- üíæ Backup k·∫øt qu·∫£ quan tr·ªçng\n",
    "- üîß Ki·ªÉm tra logs n·∫øu c√≥ l·ªói\n",
    "- üìä So s√°nh k·∫øt qu·∫£ t·ª´ nhi·ªÅu experiments kh√°c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c016c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Utility functions ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y\n",
    "\n",
    "def quick_experiment(model_type, dataset_path, experiment_name=None):\n",
    "    \"\"\"T·∫°o v√† ch·∫°y experiment nhanh\"\"\"\n",
    "    if not experiment_name:\n",
    "        experiment_name = f\"quick_{model_type}_{datetime.datetime.now().strftime('%H%M%S')}\"\n",
    "    \n",
    "    config = {\n",
    "        'description': f'Quick {model_type} experiment',\n",
    "        'models': [{\n",
    "            'name': f'{model_type}_model',\n",
    "            'type': model_type,\n",
    "            'model_name_or_path': '',\n",
    "            'device': 'auto'\n",
    "        }],\n",
    "        'datasets': [{\n",
    "            'name': 'dataset',\n",
    "            'type': 'text',\n",
    "            'data_dir': dataset_path\n",
    "        }],\n",
    "        'evaluation': {\n",
    "            'metrics': ['ndcg', 'map'],\n",
    "            'k_values': [1, 5, 10],\n",
    "            'top_k': 100\n",
    "        },\n",
    "        'output': {\n",
    "            'output_dir': './results',\n",
    "            'experiment_name': experiment_name,\n",
    "            'log_level': 'INFO'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_file = f'{experiment_name}.yaml'\n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Created config: {config_file}\")\n",
    "    print(f\"‚ñ∂Ô∏è To run: python run_experiment.py --config {config_file}\")\n",
    "    return config_file\n",
    "\n",
    "def list_recent_experiments(n=5):\n",
    "    \"\"\"Li·ªát k√™ c√°c experiment g·∫ßn ƒë√¢y\"\"\"\n",
    "    if not os.path.exists('./results'):\n",
    "        print(\"‚ùå No results directory found\")\n",
    "        return\n",
    "    \n",
    "    experiments = [d for d in Path('./results').iterdir() if d.is_dir()]\n",
    "    experiments.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    print(f\"üìä {min(n, len(experiments))} most recent experiments:\")\n",
    "    for i, exp in enumerate(experiments[:n]):\n",
    "        mtime = datetime.datetime.fromtimestamp(exp.stat().st_mtime)\n",
    "        print(f\"  {i+1}. {exp.name} ({mtime.strftime('%Y-%m-%d %H:%M')})\")\n",
    "\n",
    "print(\"üîß Utility functions loaded:\")\n",
    "print(\"  - quick_experiment(model_type, dataset_path, experiment_name=None)\")\n",
    "print(\"  - list_recent_experiments(n=5)\")\n",
    "print(\"\\nV√≠ d·ª•:\")\n",
    "print(\"  quick_experiment('sparse', './BKAI_law_data/newaibench_formatted_data/legal_data')\")\n",
    "print(\"  list_recent_experiments()\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
